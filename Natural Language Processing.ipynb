{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "What is NLP?\n",
    "- Using computers to process (analyze, understand, generate) natural human languages\n",
    "\n",
    "Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages.As such, NLP is related to the area of humanâ€“computer interaction. Many challenges in NLP involve natural language understanding -- that is, enabling computers to derive meaning from human or natural language input.\n",
    "\n",
    "Why NLP?\n",
    "- Most knowledge created by humans is unstructured text\n",
    "- Need some way to make sense of it\n",
    "- Enables quantitative analysis of text data\n",
    "\n",
    "\n",
    "Why NLTK?\n",
    "- High-quality, reusable NLP functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package hmm_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/hmm_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/ernestogiron/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASA 1\n",
      "Action 1\n",
      "Additionally 1\n",
      "Advanced 2\n",
      "Although 1\n",
      "American 2\n",
      "An 1\n",
      "Analysis 2\n",
      "Analytics 3\n",
      "April 2\n",
      "Areas 1\n",
      "Association 3\n",
      "August 1\n",
      "Because 1\n",
      "Big 1\n",
      "Board 1\n",
      "Business 2\n",
      "C. 1\n",
      "C.F 1\n",
      "CODATA 1\n",
      "Carver 1\n",
      "Century 3\n",
      "Chandra 1\n",
      "Chikio 1\n",
      "Classification 1\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tokenization\n",
    "\n",
    "What:  Separate text into units such as sentences or words\n",
    "Why:   Gives structure to previously unstructured text\n",
    "Notes: Relatively easy with English language text, not easy with some languages\n",
    "'''\n",
    "\n",
    "# \"corpus\" = collection of documents\n",
    "# \"corpora\" = plural form of corpus\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "r = requests.get(\"http://en.wikipedia.org/wiki/Data_science\")\n",
    "b = BeautifulSoup(r.text, \"lxml\")\n",
    "paragraphs = b.find(\"body\").findAll(\"p\")\n",
    "text = \"\"\n",
    "for paragraph in paragraphs:\n",
    "    text += paragraph.text + \" \"\n",
    "# Data Science corpus\n",
    "text[:500]\n",
    "\n",
    "# tokenize into sentences\n",
    "sentences = [sent for sent in nltk.sent_tokenize(text)]\n",
    "sentences[:10]\n",
    "\n",
    "# tokenize into words\n",
    "tokens = [word for word in nltk.word_tokenize(text)]\n",
    "tokens[:100]\n",
    "\n",
    "# only keep tokens that start with a letter (using regular expressions)\n",
    "import re\n",
    "clean_tokens = [token for token in tokens if re.search('^[a-zA-Z]+', token)]\n",
    "clean_tokens[:100]\n",
    "\n",
    "# count the tokens\n",
    "from collections import Counter\n",
    "c = Counter(clean_tokens)\n",
    "c.most_common(25)       # mixed case\n",
    "sorted(c.items())[:25]  # counts similar words separately\n",
    "for item in sorted(c.items())[:25]:\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 24),\n",
       " ('about', 2),\n",
       " ('academ', 1),\n",
       " ('action', 1),\n",
       " ('activ', 1),\n",
       " ('actual', 1),\n",
       " ('ad', 1),\n",
       " ('addit', 1),\n",
       " ('address', 1),\n",
       " ('advanc', 3),\n",
       " ('advantag', 1),\n",
       " ('advoc', 1),\n",
       " ('advocaci', 1),\n",
       " ('after', 1),\n",
       " ('all', 1),\n",
       " ('alon', 1),\n",
       " ('also', 1),\n",
       " ('although', 1),\n",
       " ('american', 2),\n",
       " ('an', 4),\n",
       " ('analysi', 6),\n",
       " ('analyst', 2),\n",
       " ('analyt', 6),\n",
       " ('analyz', 1),\n",
       " ('and', 49)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Stemming\n",
    "What:  Reduce a word to its base/stem form\n",
    "Why:   Often makes sense to treat multiple word forms the same way\n",
    "Notes: Uses a \"simple\" and fast rule-based approach\n",
    "       Output can be undesirable for irregular words\n",
    "       Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "       Some search engines treat words with the same stem as synonyms\n",
    "'''\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# example stemming\n",
    "stemmer.stem('charge')\n",
    "stemmer.stem('charging')\n",
    "stemmer.stem('charged')\n",
    "\n",
    "# stem the tokens\n",
    "stemmed_tokens = [stemmer.stem(t) for t in clean_tokens]\n",
    "\n",
    "# count the stemmed tokens\n",
    "c = Counter(stemmed_tokens)\n",
    "c.most_common(25)       # all lowercase\n",
    "sorted(c.items())[:25]  # some are strange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Lemmatization\n",
    "What:  Derive the canonical form ('lemma') of a word\n",
    "Why:   Can be better than stemming, reduces words to a 'normal' form.\n",
    "Notes: Uses a dictionary-based approach (slower than stemming)\n",
    "'''\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "# compare stemmer to lemmatizer\n",
    "stemmer.stem('dogs')\n",
    "lemmatizer.lemmatize('dogs')\n",
    "\n",
    "stemmer.stem('wolves') # Beter for information retrieval and search\n",
    "lemmatizer.lemmatize('wolves') # Better for text analysis\n",
    "\n",
    "stemmer.stem('is')\n",
    "lemmatizer.lemmatize('is')\n",
    "lemmatizer.lemmatize('is',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sinan', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Kevin', 'NNP'),\n",
       " ('are', 'VBP'),\n",
       " ('great', 'JJ'),\n",
       " ('teachers', 'NNS'),\n",
       " ('!', '.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Part of Speech Tagging\n",
    "What:  Determine the part of speech of a word\n",
    "Why:   This can inform other methods and models such as Named Entity Recognition\n",
    "Notes: http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "'''\n",
    "\n",
    "temp_sent = 'Sinan and Kevin are great teachers!'\n",
    "# pos_tag takes a tokenize sentence\n",
    "nltk.pos_tag(nltk.word_tokenize(temp_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  62                      data  39\n",
      "scienc  39                    Data  23\n",
      "statist  19                   science  23\n",
      "term  12                      In  16\n",
      "scientist  11                 Science  16\n",
      "method  7                     term  12\n",
      "comput  7                     Statistical  8\n",
      "busi  7                       The  7\n",
      "use  7                        scientists  7\n",
      "intern  7                     methods  6\n",
      "analysi  6                    statistics  6\n",
      "analyt  6                     business  5\n",
      "confer  6                     used  5\n",
      "journal  6                    International  5\n",
      "field  5                      analysis  4\n",
      "publish  5                    many  4\n",
      "lectur  5                     conference  4\n",
      "mine  4                       first  4\n",
      "mani  4                       Journal  4\n",
      "job  4                        field  3\n",
      "univ  4                       information  3\n",
      "first  4                      computer  3\n",
      "statistician  4               Review  3\n",
      "big  4                        Century  3\n",
      "area  3                       analytics  3\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Stopword Removal\n",
    "What:  Remove common words that will likely appear in any text\n",
    "Why:   They don't tell you much about your text\n",
    "'''\n",
    "\n",
    "# most of top 25 stemmed tokens are \"worthless\"\n",
    "c.most_common(25)\n",
    "\n",
    "# view the list of stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "sorted(stopwords)\n",
    "\n",
    "# stem the stopwords\n",
    "stemmed_stops = [stemmer.stem(t) for t in stopwords]\n",
    "\n",
    "# remove stopwords from stemmed tokens\n",
    "stemmed_tokens_no_stop = [stemmer.stem(t) for t in stemmed_tokens if t not in stemmed_stops]\n",
    "c = Counter(stemmed_tokens_no_stop)\n",
    "most_common_stemmed = c.most_common(25)\n",
    "\n",
    "# remove stopwords from cleaned tokens\n",
    "clean_tokens_no_stop = [t for t in clean_tokens if t not in stopwords]\n",
    "c = Counter(clean_tokens_no_stop)\n",
    "most_common_not_stemmed = c.most_common(25)\n",
    "\n",
    "# Compare the most common results for stemmed words and non stemmed words\n",
    "for i in range(25):\n",
    "    text_list = most_common_stemmed[i][0] + '  ' + str(most_common_stemmed[i][1]) + ' '*25\n",
    "    text_list = text_list[0:30]\n",
    "    text_list += most_common_not_stemmed[i][0] + '  ' + str(most_common_not_stemmed[i][1])\n",
    "    print(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PERSON] Kevin\n",
      "[PERSON] Sinan\n",
      "[ORGANIZATION] General Assembly\n",
      "[GPE] Washington\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Named Entity Recognition\n",
    "What:  Automatically extract the names of people, places, organizations, etc.\n",
    "Why:   Can help you to identify \"important\" words\n",
    "Notes: Training NER classifier requires a lot of annotated training data\n",
    "       Should be trained on data relevant to your task\n",
    "       Stanford NER classifier is the \"gold standard\"\n",
    "'''\n",
    "\n",
    "def extract_entities(text):\n",
    "    entities = []\n",
    "    # tokenize into sentences\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        # tokenize sentences into words\n",
    "        # add part-of-speech tags\n",
    "        # use NLTK's NER classifier\n",
    "        chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "        # parse the results\n",
    "        entities.extend([chunk for chunk in chunks if hasattr(chunk, 'label')])\n",
    "    return entities\n",
    "\n",
    "for entity in extract_entities('Kevin and Sinan are instructors for General Assembly in Washington, D.C.'):\n",
    "    print('[' + entity.label() + '] ' + ' '.join(c[0] for c in entity.leaves()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bob', 'hates', 'likes', 'sports', 'trees']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "What:  Computes \"relative frequency\" that a word appears in a document\n",
    "           compared to its frequency across all documents\n",
    "Why:   More useful than \"term frequency\" for identifying \"important\" words in\n",
    "           each document (high frequency in that document, low frequency in\n",
    "           other documents)\n",
    "Notes: Used for search engine scoring, text summarization, document clustering\n",
    "'''\n",
    "\n",
    "sample = ['Bob likes sports', 'Bob hates sports', 'Bob likes likes trees']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit_transform(sample).toarray()\n",
    "vect.get_feature_names()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit_transform(sample).toarray()\n",
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 39\n",
      "INFO:lda:vocab_size: 1497\n",
      "INFO:lda:n_words: 1874\n",
      "INFO:lda:n_topics: 10\n",
      "INFO:lda:n_iter: 500\n",
      "INFO:lda:<0> log likelihood: -21343\n",
      "INFO:lda:<10> log likelihood: -18304\n",
      "INFO:lda:<20> log likelihood: -17843\n",
      "INFO:lda:<30> log likelihood: -17596\n",
      "INFO:lda:<40> log likelihood: -17548\n",
      "INFO:lda:<50> log likelihood: -17598\n",
      "INFO:lda:<60> log likelihood: -17455\n",
      "INFO:lda:<70> log likelihood: -17323\n",
      "INFO:lda:<80> log likelihood: -17331\n",
      "INFO:lda:<90> log likelihood: -17201\n",
      "INFO:lda:<100> log likelihood: -17441\n",
      "INFO:lda:<110> log likelihood: -17433\n",
      "INFO:lda:<120> log likelihood: -17237\n",
      "INFO:lda:<130> log likelihood: -17315\n",
      "INFO:lda:<140> log likelihood: -17217\n",
      "INFO:lda:<150> log likelihood: -17144\n",
      "INFO:lda:<160> log likelihood: -17174\n",
      "INFO:lda:<170> log likelihood: -17277\n",
      "INFO:lda:<180> log likelihood: -17379\n",
      "INFO:lda:<190> log likelihood: -17262\n",
      "INFO:lda:<200> log likelihood: -17249\n",
      "INFO:lda:<210> log likelihood: -17369\n",
      "INFO:lda:<220> log likelihood: -17194\n",
      "INFO:lda:<230> log likelihood: -17159\n",
      "INFO:lda:<240> log likelihood: -17272\n",
      "INFO:lda:<250> log likelihood: -17244\n",
      "INFO:lda:<260> log likelihood: -17206\n",
      "INFO:lda:<270> log likelihood: -17215\n",
      "INFO:lda:<280> log likelihood: -17091\n",
      "INFO:lda:<290> log likelihood: -17268\n",
      "INFO:lda:<300> log likelihood: -17442\n",
      "INFO:lda:<310> log likelihood: -17299\n",
      "INFO:lda:<320> log likelihood: -17319\n",
      "INFO:lda:<330> log likelihood: -17327\n",
      "INFO:lda:<340> log likelihood: -17328\n",
      "INFO:lda:<350> log likelihood: -17276\n",
      "INFO:lda:<360> log likelihood: -17280\n",
      "INFO:lda:<370> log likelihood: -17308\n",
      "INFO:lda:<380> log likelihood: -17261\n",
      "INFO:lda:<390> log likelihood: -17188\n",
      "INFO:lda:<400> log likelihood: -17256\n",
      "INFO:lda:<410> log likelihood: -17354\n",
      "INFO:lda:<420> log likelihood: -17333\n",
      "INFO:lda:<430> log likelihood: -17372\n",
      "INFO:lda:<440> log likelihood: -17253\n",
      "INFO:lda:<450> log likelihood: -17415\n",
      "INFO:lda:<460> log likelihood: -17280\n",
      "INFO:lda:<470> log likelihood: -17413\n",
      "INFO:lda:<480> log likelihood: -17326\n",
      "INFO:lda:<490> log likelihood: -17278\n",
      "INFO:lda:<499> log likelihood: -17454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: field, areas, technical, technical areas, article data, computing data, computing, article, field statistics\n",
      "Topic 1: data, scientists, data scientists, lecture, lecture entitled statistics, data collection, entitled statistics data, analysts, collection\n",
      "Topic 2: data, science, data science, journal, 2015, data science journal, 2014, science journal, statistical learning\n",
      "Topic 3: statistical, analysis, data mining, mining, section, association, learning, 2001, research\n",
      "Topic 4: science, data, data science, statistics, term, computer, term data science, term data, used\n",
      "Topic 5: university, classification, international, publication, applications, society, definition, degree, survey\n",
      "Topic 6: information, systems, issues, digital, digital data, data driven, technology, disciplinary, librarians archivists\n",
      "Topic 7: methods, conference, analytics, international, conference data, launched, data, ecda, journal data science\n",
      "Topic 8: big data, statistician, big, term statistician, mahalanobis, lectures, indian, scientists big, scientist statistician\n",
      "Topic 9: business, term, scientist, data scientist, review, 21st, 21st century, job, century\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "LDA - Latent Dirichlet Allocation\n",
    "What:  Way of automatically discovering topics from sentences\n",
    "Why:   Much quicker than manually creating and identifying topic clusters\n",
    "'''\n",
    "# pip install lda\n",
    "import lda\n",
    "import numpy as np\n",
    "\n",
    "# Instantiate a count vectorizer with two additional parameters\n",
    "vect = CountVectorizer(stop_words='english', ngram_range=[1,3]) \n",
    "sentences_train = vect.fit_transform(sentences)\n",
    "\n",
    "# Instantiate an LDA model\n",
    "model = lda.LDA(n_topics=10, n_iter=500)\n",
    "model.fit(sentences_train) # Fit the model \n",
    "n_top_words = 10\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vect.get_feature_names())[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ', '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOWEST:\n",
      "\n",
      "not that it doesn't try .\n",
      "yet , in the end , it doesn't really matter .\n",
      "the four leads are nearly all playing the same character .\n",
      "\n",
      "HIGHEST:\n",
      "\n",
      "there's the group's unofficial leader , thurgood ( david chappelle ) , scarface ( guillermo diaz ) , brian ( jim breuer ) , and kenny ( harland williams ) .\n",
      "that is , until thurgood stumbles upon a stash of pharmaceutical marijuana being tested at the company where he works as a janitor .\n",
      "to top it off , and in a move contrasting with the tone of the rest of the film , thurgood is given a love interest , mary jane ( rachel true ) .\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "EXAMPLE: Automatically summarize a document\n",
    "'''\n",
    "\n",
    "# corpus of 2000 movie reviews\n",
    "from nltk.corpus import movie_reviews\n",
    "reviews = [movie_reviews.raw(filename) for filename in movie_reviews.fileids()]\n",
    "\n",
    "# create document-term matrix\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "dtm = tfidf.fit_transform(reviews)\n",
    "features = tfidf.get_feature_names()\n",
    "\n",
    "# find the most and least \"interesting\" sentences in a randomly selected review\n",
    "def summarize():\n",
    "    # choose a random movie review    \n",
    "    review_id = np.random.randint(0, len(reviews))\n",
    "    review_text = reviews[review_id]\n",
    "\n",
    "    # we are going to score each sentence in the review for \"interesting-ness\"\n",
    "    sent_scores = []\n",
    "    # tokenize document into sentences\n",
    "    for sentence in nltk.sent_tokenize(review_text):\n",
    "        # exclude short sentences\n",
    "        if len(sentence) > 6:\n",
    "            score = 0\n",
    "            token_count = 0\n",
    "            # tokenize sentence into words\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            # compute sentence \"score\" by summing TF-IDF for each word\n",
    "            for token in tokens:\n",
    "                if token in features:\n",
    "                    score += dtm[review_id, features.index(token)]\n",
    "                    token_count += 1\n",
    "            # divide score by number of tokens\n",
    "            sent_scores.append((score / float(token_count + 1), sentence))\n",
    "\n",
    "    # lowest scoring sentences\n",
    "    print('\\nLOWEST:\\n')\n",
    "    for sent_score in sorted(sent_scores)[:3]:\n",
    "        print (sent_score[1])\n",
    "\n",
    "    # highest scoring sentences\n",
    "    print('\\nHIGHEST:\\n')\n",
    "    for sent_score in sorted(sent_scores, reverse=True)[:3]:\n",
    "        print (sent_score[1])\n",
    "\n",
    "# try it out!\n",
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.0\n",
      "Bienvenido al salÃ³n de clases.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'es'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "TextBlob Demo: \"Simplified Text Processing\"\n",
    "Installation: pip install textblob\n",
    "'''\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "# identify words and noun phrases\n",
    "blob = TextBlob('Kevin and Sinan are instructors for General Assembly in Washington, D.C.')\n",
    "blob.words\n",
    "blob.noun_phrases\n",
    "\n",
    "# sentiment analysis\n",
    "blob = TextBlob('I hate this horrible movie. This movie is not very good.')\n",
    "blob.sentences\n",
    "blob.sentiment.polarity\n",
    "[sent.sentiment.polarity for sent in blob.sentences]\n",
    "\n",
    "# sentiment subjectivity\n",
    "TextBlob(\"I am a cool person\").sentiment.subjectivity # Pretty subjective\n",
    "TextBlob(\"I am a person\").sentiment.subjectivity # Pretty objective\n",
    "# different scores for essentially the same sentence\n",
    "print(TextBlob('Kevin and Sinan are instructors for General Assembly in Washington, D.C.').sentiment.subjectivity)\n",
    "print(TextBlob('Kevin and Sinan are instructors in Washington, D.C.').sentiment.subjectivity)\n",
    "\n",
    "# singularize and pluralize\n",
    "blob = TextBlob('Put away the dishes.')\n",
    "[word.singularize() for word in blob.words]\n",
    "[word.pluralize() for word in blob.words]\n",
    "\n",
    "# spelling correction\n",
    "blob = TextBlob('15 minuets late')\n",
    "blob.correct()\n",
    "\n",
    "# spellcheck\n",
    "Word('parot').spellcheck()\n",
    "\n",
    "# definitions\n",
    "Word('bank').define()\n",
    "Word('bank').define('v')\n",
    "\n",
    "# translation and language identification\n",
    "blob = TextBlob('Welcome to the classroom.')\n",
    "print(blob.translate(to='es'))\n",
    "blob = TextBlob('Hola amigos')\n",
    "blob.detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love Sinan! 3.0\n",
      "I hate Sinan! -3.0\n",
      "I feel nothing about Sinan! 0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Data Science Toolkit Sentiment\n",
    "Provides many different APIs for converting and getting information\n",
    "We'll use the text2sentiment API.\n",
    "'''\n",
    "# Import the necessary modules\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Sample sentences\n",
    "sentences = ['I love Sinan!', 'I hate Sinan!', 'I feel nothing about Sinan!']\n",
    "# API endpoint (i.e.the URL they ask you to send your text to)\n",
    "url = 'http://www.datasciencetoolkit.org/text2sentiment/'\n",
    "\n",
    "# Loop through the sentences\n",
    "for sentence in sentences:\n",
    "    payload = {'text': sentence} # The sentence we want the sentiment of \n",
    "    headers = {'content-type': 'application/json'} # The type of data you are sending\n",
    "    r = requests.post(url, data=json.dumps(payload), headers=headers) # Send the data\n",
    "    print (sentence, json.loads(r.text)['score']) # Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
