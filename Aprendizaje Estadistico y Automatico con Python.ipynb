{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje Estadístico y Automático\n",
    "\n",
    "A continuación encontrarán algunas guías sobre el aprendizaje estadístico y técnicas de aprendizaje automatico o machine learning que he abordado sobre varios cursos tanto presenciales como en línea.\n",
    "\n",
    "Empezaremos desde los temas mas sencillos hasta ir abordando los mas dificiles, en lo posible tanto teórico como práctico.\n",
    "\n",
    "** Recuerde que estas notas son personales y no constituyen un tutorial o curso en línea de estas disciplinas **. Dicho lo anterior recomiendo en todos los casos comprar los libros citados y tomar cursos adicionales para entender mejor los conceptos fundamentales detras de todo esto.\n",
    "\n",
    "Ahora si manos a la obra.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje Estadístico\n",
    "\n",
    "Aunque el término aprendizaje estadístico (_Statistical Learning_) es relativamente nuevo, muchos de los conceptos que lo fundamentan se desarrollaron hace mucho tiempo.\n",
    "\n",
    "> A principios del siglo XIX, **Legendre y Gauss** publicaron artículos sobre el método de los mínimos cuadrados, que implementaron la forma más antigua de lo que ahora se conoce como _regresión lineal_. El enfoque se aplicó primero \n",
    "con éxito a los problemas de la astronomía. La regresión lineal se utiliza hoy día en muchos campos de diferentes \n",
    "disciplinas.\n",
    "\n",
    "Para llevar un orden en lo que vamos aprendiendo, seguiremos el texto \"**An Introduction to Statistical Learning with Applications in R**\", de _Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani_. Este puede ser descargado de la [página del autor](http://www-bcf.usc.edu/~gareth/ISL/). \n",
    "\n",
    "![Libro ISLR](./assets/ISLCover.jpg)\n",
    "\n",
    "This book provides an introduction to statistical learning methods. It is aimed for upper level undergraduate students, masters students and Ph.D. students in the non-mathematical sciences. The book also contains a number of R labs with detailed explanations on how to implement the various methods in real life settings, and should be a valuable resource for a practicing data scientist.\n",
    "\n",
    "De igual modo utilizaremos parte del material del profesor **Elkin A. Castaño V.** que ha traducido al español gran parte del texto. Este material fue facilitado en el [Diplomado en Ciencia de Datos: Data Mining de la Universidad del Valle](http://escuelaestadistica.univalle.edu.co/diplomado-data-mining), Cali - Colombia.\n",
    "\n",
    "El Data Mining ( Minería de Datos) se presenta en la actualidad como una nueva alternativa, que permite explorar grandes bases de datos, de manera automática o semiautomática, con el objetivo de encontrar patrones repetitivos, tendencias o reglas que expliquen el comportamiento de los datos en un determinado contexto, con el fin de que puedan usarse para predecir comportamientos futuros, transformando los datos en conocimiento proactivo, para la toma de decisiones de empresas públicas y privadas, científicos, universidades, entre otros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Una Breve Historia del Aprendizaje Estadístico\n",
    "\n",
    "* Con el fin de predecir valores cualitativos, tales como si un paciente sobrevive o muere, o si el mercado de valores aumenta o disminuye, Fisher propuso _análisis discriminante lineal_ en 1936.\n",
    "\n",
    "* En la década de 1940, varios autores presentaron un enfoque alternativo, llamado _regresión logística_.\n",
    "\n",
    "* A principios de los años setenta, Nelder y Wedderburn acuñaron el término de _modelos lineales generalizados (GLIM)_ para toda una clase de métodos de aprendizaje estadístico que incluyen la regresión lineal y logística como casos especiales.\n",
    "\n",
    "* A finales de la década de 1970, había muchas más técnicas para el aprendizaje de datos. Sin embargo, eran casi exclusivamente métodos lineales, debido a que *el ajuste de relaciones no lineales era computacionalmente imposible* en ese momento. En los años ochenta, la tecnología informática había mejorado lo suficiente para que los métodos no lineales ya no fueran computacionalmente prohibitivos.\n",
    "\n",
    "* A mediados de los años ochenta, Breiman, Friedman, Olshen y Stone introdujeron _árboles de clasificación y regresión_ y estuvieron entre los primeros en demostrar la potencia de una implementación práctica detallada de los métodos, incluyendo la _validación cruzada_ para la _selección de modelos_.\n",
    "\n",
    "* Hastie y Tibshirani acuñaron el término de _modelos aditivos generalizados (GAM)_ en 1986 para una clase de extensiones _no lineales_ de los modelos lineales generalizados, y también proporcionaron una implementación práctica de software.\n",
    "\n",
    "\n",
    "Desde entonces, inspirado en la llegada del _aprendizaje automático_ (_machine learning_, el cual es un subcampo de las [ciencias de la computación](https://es.wikipedia.org/wiki/Ciencias_de_la_computaci%C3%B3n) y una rama de la [inteligencia artificial](https://es.wikipedia.org/wiki/Inteligencia_artificial) cuyo objetivo es desarrollar técnicas que permitan a las computadoras _aprender_) y otras disciplinas, el aprendizaje estadístico ha surgido como un nuevo subcampo en la Estadística, centrado en la modelación y la predicción supervisada y no supervisada. En los últimos años, el progreso en el aprendizaje estadístico ha sido marcado por la creciente disponibilidad de software potente y relativamente fácil de usar, tal como como el popular sistema [R](https://www.r-project.org/) y [Python](https://www.python.org/), los cuales son de uso libre.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quienes va dirigido estas notas\n",
    "\n",
    "Este curso está dirigido a las personas que están interesadas en el empleo de métodos estadísticos para la modelación y la predicción a partir de datos. Este grupo de personas no solamente incluye a científicos, ingenieros, analistas de datos, o analistas cuantitativos, sino también a individuos menos técnicos en campos no-cuantitativos tales como las ciencias sociales o los negocios. Se espera que el participante haya tenido por lo menos un curso elemental en Estadística. El nivel matemático del curso es modesto y no se requiere un conocimiento detallado del álgebra matricial.\n",
    "\n",
    "La comunidad de usuarios de las técnicas de aprendizaje estadístico ha venido creciendo y e incluye individuos con una gama más amplia de intereses y formaciones. Es importante que este grupo heterogéneo sea capaz de comprender los modelos, sus alcances y las fortalezas y debilidades de los diversos enfoques. Pero para este público, muchos de los detalles técnicos de los métodos de aprendizaje estadístico, como los algoritmos de optimización y las propiedades teóricas, no son de interés primordial. Estos estudiantes no necesitan una comprensión profunda de estos aspectos para convertirse en usuarios conocedores de las diversas metodologías, y para contribuir a sus campos de trabajo a través del uso de herramientas de aprendizaje estadístico.\n",
    "\n",
    "Esta introducción al aprendizaje estadístico se basa en las siguientes cuatro premisas.\n",
    "\n",
    "1. Muchos métodos de aprendizaje estadístico son relevantes y útiles en una amplia gama de disciplinas académicas y no académicas, más allá de la ciencia Estadística. \n",
    "\n",
    "2. El aprendizaje estadístico no debe ser visto como una serie de cajas negras.\n",
    "\n",
    "3. Si bien es importante saber qué trabajo realiza cada engranaje, no es necesario tener las habilidades para construir la máquina dentro de la caja.\n",
    "\n",
    "4. Suponemos que el lector está interesado en aplicar métodos de aprendizaje estadístico a problemas del mundo real. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Si desea tener una introduccíon un poco mas detallada puede acceder a los videos y archivos de presentación que soportan el curso de Statistical Learning dictados por el Dr. Hastie y el Dr. Tibshirani discuss.\n",
    "\n",
    "* [Presentación en PDF](https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/introduction.pdf)\n",
    "* [Opening Remarks and Examples](https://www.youtube.com/watch?v=2wLfFB_6SKI) (Video - duración 18:18)\n",
    "[![](https://img.youtube.com/vi/2wLfFB_6SKI/0.jpg)](https://www.youtube.com/watch?v=2wLfFB_6SKI)\n",
    "* [Supervised and Unsupervised Learning](https://www.youtube.com/watch?v=LvaTokhYnDw) (Video - duración 12:12)\n",
    "[![](https://img.youtube.com/vi/LvaTokhYnDw/0.jpg)](https://www.youtube.com/watch?v=LvaTokhYnDw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis Inteligente de Datos\n",
    "\n",
    "1. [**Regresión lineal**](Regresion%20Lineal.ipynb#Regresio%CC%81n-Lineal)\n",
    "    - [Regresión lineal simple](Regresion%20Lineal.ipynb#Regresi%C3%B3n-Lineal-Simple)\n",
    "        * [Estimando \"aprendiendo\" los Coeficientes](Regresion%20Lineal.ipynb#Estimando-%22aprendiendo%22-los-Coeficientes)\n",
    "        * [Evaluación de la exactitud de las estimaciones de coeficientes](Regresion%20Lineal.ipynb#Evaluacio%CC%81n-de-la-exactitud-de-las-estimaciones-de-coeficientes)\n",
    "        * [Evaluación de la exactitud del modelo](Regresion%20Lineal.ipynb#Evaluacio%CC%81n-de-la-exactitud-del-modelo) \n",
    "            - [Error Estándar Residual (RSE)](Regresion%20Lineal.ipynb#Error-Estándar-Residual---RSE)\n",
    "            - [Estadístico $R^2$](Regresion%20Lineal.ipynb#Estadi%CC%81stico-$R^2$)\n",
    "        * [Confianza en nuestro modelo](Regresion%20Lineal.ipynb#Confianza-en-nuestro-modelo)\n",
    "    - [Regresión lineal múltiple](Regresion%20Lineal%20Multiple.ipynb#Regresio%CC%81n-Lineal-Mu%CC%81ltiple)\n",
    "        * [Estimación de los Coeficientes de Regresión](Regresion%20Lineal%20Multiple.ipynb#Estimacio%CC%81n-de-los-Coeficientes-de-Regresio%CC%81n)\n",
    "        * [Estimación de los coeficientes utilizando scikit-learn](Regresion%20Lineal%20Multiple.ipynb#Estimaci%C3%B3n-de-los-coeficientes-utilizando-scikit-learn)\n",
    "        * [Matriz de correlación](Regresion%20Lineal%20Multiple.ipynb#Matriz-de-correlaci%C3%B3n)\n",
    "        * [Algunas Preguntas Importantes](Regresion%20Lineal%20Multiple.ipynb#Algunas-Preguntas-Importantes)\n",
    "            - [¿Existe una relación entre la respuesta y los predictores?](Regresion%20Lineal%20Multiple.ipynb#%C2%BFExiste-una-relacio%CC%81n-entre-la-respuesta-y-los-predictores?)\n",
    "            - [El Ajuste del Modelo](Regresion%20Lineal%20Multiple.ipynb#El-Ajuste-del-Modelo)\n",
    "            - [Predicciones](Regresion%20Lineal%20Multiple.ipynb#Predicciones)\n",
    "        * [Otras consideraciones en el modelo de regresión](Regresion%20Lineal%20Multiple.ipynb#Otras-consideraciones-en-el-modelo-de-regresio%CC%81n)\n",
    "             - [Predictores cualitativos](Regresion%20Lineal%20Multiple.ipynb#Predictores-cualitativos)\n",
    "             - [Predictores cualitativos con sólo dos niveles](Regresion%20Lineal%20Multiple.ipynb#Predictores-cualitativos-con-so%CC%81lo-dos-niveles)\n",
    "             - [Creando variables dummy manualmente](Regresion%20Lineal%20Multiple.ipynb#Creando-variables-dummy-manualmente)\n",
    "             - [Predictores cualitativos con más de dos niveles](Regresion%20Lineal%20Multiple.ipynb#Predictores-cualitativos-con-ma%CC%81s-de-dos-niveles)\n",
    "    - [Extensiones del modelo lineal](Regresion%20Lineal%20Multiple.ipynb#Extensiones-del-modelo-lineal)\n",
    "         * [Eliminación del supuesto de aditividad](Regresion%20Lineal%20Multiple.ipynb#Eliminacio%CC%81n-del-supuesto-de-aditividad)\n",
    "         * [Interacción entre variables cualitativas y cuantitativas](Regresion%20Lineal%20Multiple.ipynb#Interacci%C3%B3n-entre-variables-cualitativas-y-cuantitativas)\n",
    "         * [Relaciones no lineales](Regresion%20Lineal%20Multiple.ipynb#Relaciones-no-lineales)\n",
    "    - [Problemas potenciales](Regresion%20Lineal%20Multiple.ipynb#Problemas-potenciales)\n",
    "         - [No linealidad de las relaciones respuesta-predictor](Regresion%20Lineal%20Multiple.ipynb#1.-No-linealidad-de-los-datos)\n",
    "         - [Correlación de los términos de error](Regresion%20Lineal%20Multiple.ipynb#2.-Correlacio%CC%81n-de-los-te%CC%81rminos-de-error)\n",
    "         - [Variación no constante de los términos de error](Regresion%20Lineal%20Multiple.ipynb#3.-Variacio%CC%81n-no-constante-de-los-te%CC%81rminos-de-error)\n",
    "         - [Valores atípicos](Regresion%20Lineal%20Multiple.ipynb#4.-Valores-ati%CC%81picos)\n",
    "         - [Puntos de alta influncia o apalancamiento - Leverage](Regresion%20Lineal%20Multiple.ipynb#5.-Puntos-de-alta-influncia-o-apalancamiento---Leverage)\n",
    "         - [Colinealidad](Regresion%20Lineal%20Multiple.ipynb#6.-Colinealidad)\n",
    "             - [Fáctor de inflación de la varianza - VIF](Regresion%20Lineal%20Multiple.ipynb#F%C3%A1ctor-de-inflacio%CC%81n-de-la-varianza---VIF) \n",
    "    - [Algunos gráficos de Ayuda](Regresion%20Lineal%20Multiple.ipynb#Gráficos-similares-a-la-salida-de-R)\n",
    "        * [Gráfico de Residuales](Regresion%20Lineal%20Multiple.ipynb#Gr%C3%A1fico-de-Residuales)\n",
    "        * [Gráfico de Cuantiles - QQ](Regresion%20Lineal%20Multiple.ipynb#Gr%C3%A1fico-de-Cuantiles---QQ)\n",
    "        * [Gráfico de Escala - Localización](Regresion%20Lineal%20Multiple.ipynb#Gr%C3%A1fico-de-Escala---Localizaci%C3%B3n)\n",
    "        * [Gráfico de influencias o apalancamiento (Leverage)](Regresion%20Lineal%20Multiple.ipynb#Gr%C3%A1fico-de-influencias-o-apalancamiento---Leverage)\n",
    "2. [**Clasificación**](Clasificacion.ipynb)\n",
    "    - [Una visión general de la clasificación](Clasificacion.ipynb#Clasificaci%C3%B3n)\n",
    "    - [¿Por qué no la regresión lineal?](Clasificacion.ipynb#%C2%BFPorque%CC%81-no-usar-Regresio%CC%81n-Lineal?)\n",
    "    - [Regresión Logística](Clasificacion.ipynb#Regresio%CC%81n-Logi%CC%81stica)\n",
    "        * [El Modelo Logístico](Clasificacion.ipynb#El-Modelo-Logi%CC%81stico)\n",
    "        * [Estimación de los Coeficientes de Regresión](Clasificacion.ipynb#Estimacio%CC%81n-de-los-Coeficientes-de-Regresio%CC%81n)\n",
    "        * [Predicciones](Clasificacion.ipynb#Predicciones)\n",
    "        * [Regresión Logística Múltiple](Clasificacion.ipynb#Regresio%CC%81n-Logi%CC%81stica-Mu%CC%81ltiple)\n",
    "    - [Análisis Discriminante Lineal](Clasificacion.ipynb#Ana%CC%81lisis-Discriminante-Lineal---LDA)\n",
    "        * [Uso del teorema de Bayes para la clasificación](Clasificacion.ipynb#Uso-del-teorema-de-Bayes-para-la-clasificacio%CC%81n)\n",
    "        * [Análisis Discriminante Lineal para p = 1](Clasificacion.ipynb#Ana%CC%81lisis-Discriminante-Lineal-para-p-=-1)\n",
    "        * [Análisis Discriminante Lineal para p>1](Clasificacion.ipynb#Ana%CC%81lisis-Discriminante-Lineal-para-p%3E1)\n",
    "        * [La curva ROC](Clasificacion.ipynb#La-curva-ROC)\n",
    "    - [Análisis Discriminante Cuadrático](Clasificacion.ipynb#Ana%CC%81lisis-Discriminante-Cuadra%CC%81tico)\n",
    "    - [Ejemplo con K-Nearest Neighbors](Clasificacion.ipynb#K-Nearest-Neighbors)\n",
    "3. [**Aprendizaje no supervisado**](Aprendizaje%20no%20supervisado.ipynb#Aprendizaje-no-supervisado)\n",
    "    - [El desafío del aprendizaje sin supervisión](Aprendizaje%20no%20supervisado.ipynb#El-desafi%CC%81o-del-aprendizaje-sin-supervisio%CC%81n)\n",
    "    - [Análisis de componentes principales](Aprendizaje%20no%20supervisado.ipynb#Ana%CC%81lisis-de-componentes-principales)\n",
    "        * [¿Qué son las componentes principales?](Aprendizaje%20no%20supervisado.ipynb#%C2%BFQue%CC%81-son-las-componentes-principales?)\n",
    "        * [Otra Interpretación de las Componentes Principales](Aprendizaje%20no%20supervisado.ipynb#Otra-Interpretacio%CC%81n-de-las-Componentes-Principales)\n",
    "        * [Más sobre el PCA](Aprendizaje%20no%20supervisado.ipynb#Ma%CC%81s-sobre-el-PCA)\n",
    "            - [La escala de las variables](Aprendizaje%20no%20supervisado.ipynb#La-escala-de-las-variables)\n",
    "            - [Unicidad de las Componentes Principales](Aprendizaje%20no%20supervisado.ipynb#Unicidad-de-las-Componentes-Principales)\n",
    "            - [La proporción de la varianza explicada](Aprendizaje%20no%20supervisado.ipynb#La-proporcio%CC%81n-de-la-varianza-explicada)\n",
    "            - [Cuántas componentes principales usar](Aprendizaje%20no%20supervisado.ipynb#Cua%CC%81ntas-componentes-principales-usar)\n",
    "            - [Otros usos de las componentes principales](Aprendizaje%20no%20supervisado.ipynb#Otros-usos-de-las-componentes-principales)\n",
    "    - [Métodos de agrupamiento (Clustering)](Metodos%20de%20agrupacion%20o%20Clustering.ipynb#Me%CC%81todos-de-agrupacio%CC%81n---Clustering)\n",
    "        * [Agrupación de K-Means](Metodos%20de%20agrupacion%20o%20Clustering.ipynb#Agrupacio%CC%81n-de-K-Means)\n",
    "        * [Agrupación jerárquica](Metodos%20de%20agrupacion%20o%20Clustering.ipynb#Agrupacio%CC%81n-jera%CC%81rquica)\n",
    "            - [Interpretación de un dendrograma](Metodos%20de%20agrupacion%20o%20Clustering.ipynb#Interpretacio%CC%81n-de-un-dendrograma)\n",
    "            - [El algoritmo de agrupación jerárquica](Metodos%20de%20agrupacion%20o%20Clustering.ipynb#El-algoritmo-de-agrupacio%CC%81n-jera%CC%81rquica)\n",
    "            - [Elección de la medida de disimilitud](Metodos%20de%20agrupacion%20o%20Clustering.ipynb#Eleccio%CC%81n-de-la-medida-de-disimilitud)\n",
    "        * [Aspectos prácticos del agrupamiento](Metodos%20de%20agrupacion%20o%20Clustering.ipynb#Aspectos-pra%CC%81cticos-del-agrupamiento)\n",
    "            - [Pequeñas decisiones con grandes consecuencias](Metodos%20de%20agrupacion%20o%20Clustering.ipynb#Pequen%CC%83as-decisiones-con-grandes-consecuencias)\n",
    "            - [Validación de los clusters obtenidos](Metodos%20de%20agrupacion%20o%20Clustering.ipynb#Validacio%CC%81n-de-los-clusters-obtenidos)\n",
    "            - [Otras consideraciones en la agrupación](Metodos%20de%20agrupacion%20o%20Clustering.ipynb#Otras-consideraciones-en-la-agrupacio%CC%81n)\n",
    "            - [Un enfoque recomendado para interpretar los resultados del agrupamiento](Metodos%20de%20agrupacion%20o%20Clustering.ipynb#Un-enfoque-recomendado-para-interpretar-los-resultados-del-agrupamiento)\n",
    "        * [Un ejemplo de agrupamiento con datos reales](Metodos%20de%20agrupacion%20o%20Clustering.ipynb#Un-Ejemplo-con-datos-reales)\n",
    "4. [**Métodos basados en Árboles**](Metodos%20basados%20en%20arboles.ipynb)\n",
    "    - [Arboles de regresión](Metodos%20basados%20en%20arboles.ipynb#Arboles-de-regresi%C3%B3n)\n",
    "        * [Realizando predicciones sobre nuevos juegos de datos](Metodos%20basados%20en%20arboles.ipynb#Realizando-predicciones-sobre-nuevos-juegos-de-datos)\n",
    "    - [Árboles de Clasificación](Metodos%20basados%20en%20arboles.ipynb#%C3%81rboles-de-Clasificaci%C3%B3n)\n",
    "        * [Criterio de división en los árboles de clasificación](Metodos%20basados%20en%20arboles.ipynb#Criterio-de-divisi%C3%B3n-en-los-%C3%A1rboles-de-clasificaci%C3%B3n)\n",
    "        * [Manipulando predictores categóricos](Metodos%20basados%20en%20arboles.ipynb#Manipulando-predictores-categ%C3%B3ricos)\n",
    "        * [Ejemplo de Árbol de Clasificación](Metodos%20basados%20en%20arboles.ipynb#Ejemplo-de-%C3%81rbol-de-Clasificaci%C3%B3n)\n",
    "        * [Otro ejemplo de Arboles de Clasificación con los datos de la base de datos IRIS](Metodos%20basados%20en%20arboles.ipynb#Otro-ejemplo-de-Arboles-de-Clasificaci%C3%B3n-con-los-datos-de-la-base-de-datos-IRIS)\n",
    "        * [Ejemplo de Clasificación utilizando los datos de Aduanas](Metodos%20basados%20en%20arboles.ipynb#Ejemplo-de-Clasificaci%C3%B3n-utilizando-los-datos-de-Aduanas)\n",
    "        * [Determinando donde o cuando podar](Metodos%20basados%20en%20arboles.ipynb#Determinando-donde-o-cuando-podar)\n",
    "    - [Algunas consideraciones de los Árboles de Decisión](Metodos%20basados%20en%20arboles.ipynb#Algunas-consideraciones-de-los-%C3%81rboles-de-Decisi%C3%B3n)\n",
    "    - [Ejemplo de Áboles de Decisión en R](Metodos%20basados%20en%20arboles.ipynb#Ejemplo-de-%C3%81boles-de-Decisi%C3%B3n-en-R)\n",
    "    - [Ejemplo de Árboles de Decisión en R con libreria rpart y C50](Metodos%20basados%20en%20arboles.ipynb#Ejemplo-de-%C3%81rboles-de-Decisi%C3%B3n-en-R-con-libreria-rpart-y-C50)\n",
    "    - [Random Forest y Bootstrap Aggregation (Bagging)](Random%20Forest.ipynb)\n",
    "        * [Ejemplo con los datos del Titanic](Random%20Forest.ipynb#Ejemplo-con-los-datos-del-Titanic)\n",
    "        * [Parametros para tener en cuenta y probar en el clasificador](Random%20Forest.ipynb#Parametros-para-tener-en-cuenta-y-probar-en-el-clasificador)\n",
    "        * [Boosting](Random%20Forest.ipynb#Boosting)\n",
    "        * [Bagging y Random Forests en R](Random%20Forest.ipynb#Bagging-y-Random-Forests-en-R)\n",
    "5. [**Maquínas de Véctores Soporte - (SVM)**](SVM.ipynb)\n",
    "    - [Maximal Margin Classifier]()\n",
    "    - [Support Vector Classifier]()\n",
    "    - [Kernels and Support Vector Machines]()\n",
    "6. [**Minería de texto**](TextMining.ipynb)\n",
    "    - [Vectorización](TextMining.ipynb#Vectorizaci%C3%B3n)\n",
    "    - [Bag of Words](TextMining.ipynb#Bag-of-Words)\n",
    "    - [Procesamiento y análisis de textos](TextMining.ipynb#Procesamiento-y-an%C3%A1lisis-de-textos)\n",
    "    - [¿Que son los NLP?](TextMining.ipynb#%C2%BFQue-son-los-NLP?)\n",
    "    - [¿Por que utilizar NLP?](TextMining.ipynb#%C2%BFPor-que-utilizar-NLP?)\n",
    "    - [Procesamiento utilizando NLTK](TextMining.ipynb#Procesamiento-utilizando-NLTK)\n",
    "        * [Procesamiento de texto crudo](TextMining.ipynb#Procesamiento-de-texto-crudo)\n",
    "        * [Tokenization](TextMining.ipynb#Tokenization)\n",
    "        * [Otro ejemplo de vectorización o tokenization](TextMining.ipynb#Otro-ejemplo-de-vectorizaci%C3%B3n-o-tokenization)\n",
    "        * [Proceso de textos crudos provenientes de internet o con formato HTML](TextMining.ipynb#Proceso-de-textos-crudos-provenientes-de-internet-o-con-formato-HTML)\n",
    "        * [Utilizando expresiones regulares](TextMining.ipynb#Utilizando-expresiones-regulares)\n",
    "        * [Normalización de textos](TextMining.ipynb#Normalizaci%C3%B3n-de-textos)\n",
    "        * [Otro ejemplo de Stemming](TextMining.ipynb#Otro-ejemplo-de-Stemming)\n",
    "        * [Etiquetado o Tagging](TextMining.ipynb#Etiquetado-o-Tagging)\n",
    "        * [Clasificando Texto](TextMining.ipynb#Clasificando-Texto)\n",
    "        * [Lemmatization](TextMining.ipynb#Lemmatization)\n",
    "        * [Stopword Removal](TextMining.ipynb#Stopword-Removal)\n",
    "        * [Named Entity Recognition](TextMining.ipynb#Named-Entity-Recognition)\n",
    "        * [Term Frequency - Inverse Document Frequency (TF-IDF)](TextMining.ipynb#Term-Frequency---Inverse-Document-Frequency----TF-IDF)\n",
    "        * [LDA - Latent Dirichlet Allocation](TextMining.ipynb#LDA---Latent-Dirichlet-Allocation)\n",
    "        * [EXAMPLE: Automatically summarize a document](TextMining.ipynb#EXAMPLE:-Automatically-summarize-a-document)\n",
    "        * [Simplified Text Processing](TextMining.ipynb#Simplified-Text-Processing)\n",
    "        * [Data Science Toolkit Sentiment](TextMining.ipynb#Data-Science-Toolkit-Sentiment)\n",
    "    - [Uso de la librería Gensim](TextMining.ipynb#Uso-de-la-librer%C3%ADa-Gensim)\n",
    "        * [Corpus Streaming – Un documento a la vez](TextMining.ipynb#Corpus-Streaming-%E2%80%93-Un-documento-a-la-vez)\n",
    "    - [Ejemplo de Análisis de Sentimientos en Competencia Kaggle](TextMining.ipynb#Ejemplo-de-An%C3%A1lisis-de-Sentimientos-en-Competencia-Kaggle)\n",
    "    - [Graficando Texto - WorldCloud](TextMining.ipynb#Gr%C3%A1ficando-Textos)\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Próximos temas...\n",
    "A continuación se presentan algunos de los temas importantes ó secciones que se irán agregando poco a poco.\n",
    "Estos temas corresponden a las clases magistrales y el libro guía de ISLR mencionado al comienzo. Stay tune!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 5: Resampling Methods**\n",
    "\n",
    "    Estimating Prediction Error and Validation Set Approach\n",
    "    K-fold Cross-Validation\n",
    "    Cross-Validation: The Right and Wrong Ways\n",
    "    The Bootstrap\n",
    "    More on the Bootstrap\n",
    "    Lab: Cross-Validation\n",
    "    Lab: The Bootstrap\n",
    "\n",
    "**Chapter 6: Linear Model Selection and Regularization**\n",
    "\n",
    "    Linear Model Selection and Best Subset Selection\n",
    "    Forward Stepwise Selection\n",
    "    Backward Stepwise Selection\n",
    "    Estimating Test Error Using Mallow’s Cp, AIC, BIC, Adjusted R-squared\n",
    "    Estimating Test Error Using Cross-Validation\n",
    "    Shrinkage Methods and Ridge Regression\n",
    "    The Lasso\n",
    "    Tuning Parameter Selection for Ridge Regression and Lasso\n",
    "    Dimension Reduction\n",
    "    Principal Components Regression and Partial Least Squares\n",
    "    Lab: Best Subset Selection\n",
    "    Lab: Forward Stepwise Selection and Model Selection Using Validation Set\n",
    "    Lab: Model Selection Using Cross-Validation\n",
    "    Lab: Ridge Regression and Lasso\n",
    "\n",
    "**Chapter 7: Moving Beyond Linearity (slides, playlist)**\n",
    "\n",
    "    Polynomial Regression and Step Functions\n",
    "    Piecewise Polynomials and Splines\n",
    "    Smoothing Splines\n",
    "    Local Regression and Generalized Additive Models\n",
    "    Lab: Polynomials\n",
    "    Lab: Splines and Generalized Additive Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
